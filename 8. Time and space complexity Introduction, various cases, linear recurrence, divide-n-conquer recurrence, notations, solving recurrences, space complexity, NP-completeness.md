### Time and Space Complexity

Understanding **time** and **space complexity** is crucial for evaluating the efficiency of algorithms. These complexities help us determine the computational resources (time and memory) that an algorithm will need in the worst, best, and average cases.

---

### **1. Introduction to Time and Space Complexity**

**Time Complexity** measures the amount of time an algorithm takes to run relative to the size of the input (usually denoted as *n*). It answers the question, "How does the runtime of an algorithm grow as the input size increases?"

**Space Complexity** measures the amount of memory space an algorithm needs relative to the input size. It answers the question, "How much memory does the algorithm require as the input size grows?"

Both time and space complexities are generally expressed using **Big-O notation**, which describes the asymptotic behavior of an algorithm, ignoring constant factors and lower-order terms.

---

### **2. Various Cases in Time Complexity**

- **Best Case (O)**: The time complexity when the algorithm performs the minimum possible number of operations (e.g., already sorted data).
  
- **Worst Case (O)**: The time complexity when the algorithm performs the maximum possible number of operations (e.g., reversed sorted data).

- **Average Case (O)**: The expected time complexity for a random input, assuming a uniform distribution of input data.

#### **Examples of Big-O Classifications:**
- **O(1)**: Constant time – the algorithm takes the same amount of time regardless of input size.
- **O(n)**: Linear time – the algorithm's time grows linearly with the input size.
- **O(n²)**: Quadratic time – the algorithm’s time grows quadratically with the input size.
- **O(log n)**: Logarithmic time – the algorithm’s time grows logarithmically with the input size.

---

### **3. Linear Recurrence Relation**

A **linear recurrence relation** is an equation that defines a sequence based on its previous terms. In the context of algorithms, recurrence relations are used to describe the time complexity of divide-and-conquer algorithms.

For example:
- **T(n) = 2T(n/2) + O(n)**

This relation is used in algorithms like **Merge Sort** and **Quick Sort**, where:
- **T(n)** is the time complexity for an input of size *n*.
- The algorithm divides the problem into two smaller sub-problems, each of size *n/2*.
- The term **O(n)** represents the work done outside the recursive calls, like merging or partitioning.

---

### **4. Divide-and-Conquer Recurrence**

Divide-and-conquer algorithms work by breaking the problem into smaller sub-problems, solving them recursively, and combining their results. The time complexity of such algorithms often follows a recurrence relation.

**General Divide-and-Conquer Recurrence:**

- **T(n) = aT(n/b) + O(n^d)**

Where:
- **a** is the number of sub-problems.
- **n/b** is the size of each sub-problem.
- **O(n^d)** is the time complexity of combining the sub-problems.

For example:
- **Merge Sort**: T(n) = 2T(n/2) + O(n)
- **Quick Sort**: T(n) = 2T(n/2) + O(n) (in average case)
- **Strassen's Matrix Multiplication**: T(n) = 7T(n/2) + O(n²)

---

### **5. Solving Recurrences**

There are several methods to solve recurrence relations and determine time complexities. Common methods include:

#### **a) Substitution Method**
- Guess the form of the solution and prove it by induction.
- For example, in Merge Sort:
  
  **T(n) = 2T(n/2) + O(n)**
  
  Guess: T(n) = O(n log n).
  Prove by induction.

#### **b) Recursion Tree Method**
- Draw a tree where each node represents a sub-problem.
- Add up the costs at each level of the tree and find the total cost.
  
  For Merge Sort, the recursion tree would look like:
  - Level 0: O(n)
  - Level 1: O(n/2) + O(n/2)
  - Level 2: O(n/4) + O(n/4) + O(n/4) + O(n/4)
  - Total: O(n log n)

#### **c) Master Theorem**
- The master theorem provides a straightforward way to solve recurrences of divide-and-conquer algorithms.
  
  For recurrences of the form:
  **T(n) = aT(n/b) + O(n^d)**

  The solution depends on the relationship between **a**, **b**, and **d**:
  
  - If **a > b^d**, then **T(n) = O(n^log_b(a))**
  - If **a = b^d**, then **T(n) = O(n^d log n)**
  - If **a < b^d**, then **T(n) = O(n^d)**

  For **T(n) = 2T(n/2) + O(n)**, apply the master theorem:
  - **a = 2**, **b = 2**, and **d = 1**.
  - Since **a = b^d**, the time complexity is **O(n log n)**.

---

### **6. Notations**

There are several notations used to express the time and space complexity of algorithms:

- **Big-O (O)**: Describes the upper bound of the time complexity. It provides the worst-case scenario.
  
  Example: O(n²) means the algorithm will take at most proportional to n² time in the worst case.

- **Omega (Ω)**: Describes the lower bound of the time complexity. It provides the best-case scenario.

  Example: Ω(n) means the algorithm will take at least proportional to n time.

- **Theta (Θ)**: Describes the exact bound of the time complexity. It provides both upper and lower bounds.

  Example: Θ(n log n) means the algorithm will take exactly proportional to n log n time.

- **Little-O (o)**: Describes a strict upper bound. If an algorithm is O(n²), it means that the algorithm will take less than or equal to n² time in the worst case.

  Example: o(n²) means the algorithm's time complexity grows strictly slower than n².

---

### **7. Space Complexity**

Space complexity measures how much memory an algorithm uses relative to the size of the input. It considers:

- **Auxiliary Space**: The extra space used by the algorithm, excluding the space for the input data.
- **Total Space**: The total memory used by the algorithm, including both the input and auxiliary space.

#### **Example of Space Complexity:**
- **Merge Sort** requires **O(n)** space for the temporary arrays used during merging, so its space complexity is **O(n)**.
- **Quick Sort**, while using recursion, has **O(log n)** space complexity for the call stack.

---

### **8. NP-Completeness**

**NP-completeness** is a class of problems in computational theory. It deals with problems for which no polynomial-time solutions are known, and the problems are believed to be hard to solve efficiently.

- **NP (Non-deterministic Polynomial time)**: A class of problems for which a solution can be verified in polynomial time. However, it is not known whether these problems can be solved in polynomial time.
  
- **NP-Complete Problems**: These are the hardest problems in NP, meaning that if one NP-complete problem can be solved in polynomial time, all NP problems can be solved in polynomial time.

  **Examples of NP-complete problems:**
  - **Travelling Salesman Problem (TSP)**
  - **Knapsack Problem**
  - **3-SAT Problem**

  **The significance**: If an NP-complete problem is solved efficiently (i.e., in polynomial time), then all problems in NP can also be solved efficiently. This leads to the famous **P ≠ NP** question, which asks whether every problem whose solution can be verified in polynomial time can also be solved in polynomial time.

---

### **Summary Table**

| Concept                | Description                                                                                             | Example Problem  |
|------------------------|---------------------------------------------------------------------------------------------------------|------------------|
| **Best Case**           | Time complexity when the algorithm performs the least amount of work.                                     | O(1) (insertion sort) |
| **Worst Case**          | Time complexity when the algorithm performs the most work.                                              | O(n²) (selection sort) |
| **Average Case**        | Expected time complexity for random inputs.                                                             | O(n log n) (quick sort) |
| **Linear Recurrence**   | Defines a sequence where each term is a linear function of its previous terms.                         | Merge Sort (T(n) = 2T(n/2) + O(n)) |
| **Master Theorem**      | A method to solve recurrences for divide-and-conquer algorithms.                                         | Merge Sort, Quick Sort |
| **Big-O**               | Upper bound on time complexity (worst-case).                                                             | O(n log n) (merge sort) |
| **Omega (Ω)**           | Lower bound on time complexity (best-case).                                                             | Ω(n) (linear search) |
| **Theta (Θ)**           | Exact bound on time complexity (best and worst-case).                                                   | Θ(n log n) (merge sort) |
| **NP-Complete**         | Class of problems that are difficult to solve but easy to verify.                                       | Travelling Salesman Problem |

